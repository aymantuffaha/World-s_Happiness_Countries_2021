---
title: "WorldHappiness_Report_2021"
author: "Ayman Tuffaha"
date: "9/4/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# import libraries
library(tidyverse)
library(caret)
library(DescTools)
library(ggcorrplot)
# import data set
data <- read.csv("UpdatedDataSet.csv")
knitr::opts_chunk$set(echo = TRUE)
```

## About this document (R Markdown Document)
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# Introduction
This project part of the capstone project of the EdX course ‘HarvardX: PH125.9x Data Science: Capstone’. It's created to take a deep analysis and to employs regression analysis in order to study the happiness of countries, and check the effect of particular factors of social society affects the happiness score, like population, generosity, GDP per capita, social support, health life expectancy, and freedom to make life choices.

The dataset I used at this project include happiness index for the year 2021 according to UN. 
[Happiness Index according to 2021 data] (https://www.kaggle.com/muhammedabdulazeem/worlds-happiness-countries-2021) and alos I have used [World Happiness Report 2021] (https://www.kaggle.com/ajaypalsinghlo/world-happiness-report-2021?select=world-happiness-report.csv)
The data being used has 151 countries listed with 10 columns. These columns are happiness rank, country name, overall happiness score, and seven factors that aid in defining the happiness score: population, GDP per capita, Social Support, Life Expectancy, Freedom, Generosity, and Perceptions of corruption.

Some Notes:
1- Happiness Rank(lower the value, better the happiness rank)
2- Happiness score(higher the value, better the happiness score. Ranges from 0-8)
3- Population of that particular country.

The report is organized as follows: second, the data set is dissected and visualized in the **Data** section. Third, the data is partitioned and the three modeling methods are shown in the **Methods** section. Fourth, the three models are compared in the **Results** section. Finally, the **Conclusion** section summarizes and compares the results.


A glimpse of our data is shown below:

```{r glimpse_data, echo=FALSE}
glimpse(data)
```

## Data
The qualitative structure and the general relationship of the data is described above. The data found here is quantitatively normal. A histogram of the happiness scores shows an approximately normal distribution and descriptive statistics support this claim.

```{r hist_score, echo=FALSE}
hist(data$score, freq=TRUE, col="black", border="white", 
     main="2021 Happiness Scores", xlab="Score", ylab="Count")
```

```{r summary_score, echo=TRUE}
summary(data$score)
```

The histogram is right-leaning. This can be shown quantitively by comparing the mean score (`r mean(data$Score)`) and the median score (`r median(data$Score)`). A mean greater than the median signifies that there are more values larger that the middle. The range of scores is (2.853, 7.769). The three highest and lowest observations are displayed below: 

```{r low_high_data, echo=FALSE}
data %>% filter(happinessRank <=3) %>% select(happinessRank, country, score)
data %>% filter(happinessRank >= 150) %>% select(happinessRank, country, score)
```

Remember that low happiness scores tend to be more like miserable societies. This means **lower happiness scores tend to have lower factor scores**. Thus, through deduction, high factor scores have a positive effect and low factor scores have a negative effect on the happiness score. Ultimately, determining the correlation of each factor to each other is of utmost interest. The `ggcorrplot` library is used to probe this:

```{r corr_plot, echo=FALSE}
temp <- data[, c(3,5,6,7,8,4,9,10)]
cormat <- signif(cor(temp), 2)
ggcorrplot(cormat)
```

All factors are at least somewhat related (above zero) with the partial exception of `pop2021` and `Generosity` and `Perceptions.of.corruption` . These three factors seems to have less of an effect on the happiness score with a correlation of around zero, thus, *removing the `pop2021` and `Generosity` and `Perceptions.of.corruption`term in the model may improve the accuracy*. The relationships between specific variables can be portrayed more directly by plotting them and determining the line of best fit. For instance, compare `Score` versus `Generosity` and `Score` versus `GDP per capita`:

```{r score_vs_gen, echo=FALSE}
ggplot(data = data, aes(x = score, Generosity)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```

```{r score_vs_gdp, echo=FALSE}
ggplot(data = data, aes(x = score, Logged.GDP.per.capita)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```
Let's compare `Score` versus poopulation `pop2021` and `Score` versus `GDP per capita`:

```{r score_vs_population, echo=FALSE}
ggplot(data = data, aes(x = score, pop2021)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```

```{r score_vs_gdp, echo=FALSE}
ggplot(data = data, aes(x = score, Logged.GDP.per.capita)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```

Let's compare `Score` versus `Perceptions.of.corruption` and `Score` versus `GDP per capita`:

```{r score_vs_perceptions of corruption, echo=FALSE}
ggplot(data = data, aes(x = score, Perceptions.of.corruption)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```

```{r score_vs_gdp, echo=FALSE}
ggplot(data = data, aes(x = score, Logged.GDP.per.capita)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE)
```
A horizontal line of best fit shows the lack of correlation between score and generosity, population, and Perceptions of corruption.

Note that the correlation between score and generosity on the correlation map was close to zero. Conversely, the plot of Score and GDP per capita clearly shows a positive correlation between the two: When `GDP per capita` increases, `Score` also increases. Also note the line of best fit for generosity has relatively high confidence bands compared to the GDP per capita plot, realizing it is a rather unstable parameter.

Visualizing data patterns may offer clues of intriguing routes to choose when performing analysis but it doesn't provide any noteworthy prediction capabilities. This is where machine learning techniques begin to excel. In the next section, we build several models to determine the best way to predict a happiness score.



## Methods

### Model 1: The Sum of Factors

The first model is seemingly the most obvious and was proposed on a [discussion forum](https://www.kaggle.com/unsdsn/world-happiness/discussion/35141) from the Kaggle website. It states that the "perfect" prediction model is to simply take the sum of all factors as the happiness score. This model is attempted with one caveat: a "standard dystopia score" was discovered in earlier happiness reports and was given the value 1.85. This value is added to our predicted scores as well because each factor is a ranking of how much *better* the country is than the standard dystopia. Note the use Root Mean Square Error (RMSE) as a success indicator. This choice is further explained in the results section. 

```{r sum_factors_model, echo=TRUE, warning=FALSE}
# find  predicted score by sum method and calculate the corresponding RMSE
sum_model <- data %>% mutate(pred_score = pop2021 +
                               Logged.GDP.per.capita +
                               Social.support +
                               Healthy.life.expectancy +
                               Freedom.to.make.life.choices + 
                               Generosity + 
                               Perceptions.of.corruption + 
                               1.85, 
                             RMSE = RMSE(score, pred_score))
# show top results of the summation model
sum_model %>%
  filter(happinessRank <= 5) %>%
  select(happinessRank, country, score, pred_score, RMSE)
```

```{r mod1_rmse_save, include=FALSE}
# save RMSE for the first model
mod1_rmse <- RMSE(sum_model$score, sum_model$pred_score)
```

All predicted scores shows the same RMSE! In addition to the dystopian standard score, previous Happiness Report data sets also have a "dystopian residual" that contributes to the happiness score. Since the residual is not shown or described in this data set, it is deemed inappropriate to introduce it ourselved into the model. Although, without this value, the data set does seem incomplete. In spite of its absence, the dystopian residual column is calculated and a snippet of these are shown below. This is calculated under the assumption the summation model is perfect. Simply subtracting all factors and the standard dystopia value from the happiness score will yield the residual. There is no mention of the missing residual in any recent discussions to the 2018 or 2019 dataset, therefore it is omitted in subsequent modeling techniques. We can conclude that the sum of factors model is not perfectly accurate as portrayed in the discussion boards; it has a RMSE of 0.5280065.

```{r }
# calculate the missing dystopian residuals
sum_model <- sum_model %>% mutate(residual = score - pred_score)
# show top results of the summation model
sum_model %>%
  filter(happinessRank <= 5) %>%
  select(happinessRank,  country, score, pred_score, RMSE, residual)
```


### Model 2: The 2021 GLM Model

Before our first linear regression model is applied, the data must be partitioned into a training and test set. This step is common when employing machine learning algorithms that require a check on the goodness of fit. It reduces the probability of overfitting to our training data at the expense of our prediction model. This was not completed for our sum of factors model because a model did not need to be trained, the equation was simply `sum(data$[factors])`.

The Happiness Report has over 150 country observations and seven factors in which we will condition our model. Given the relatively low number of observations compared to the amount of factors, the model may have a tendency to overfit to the training data by overweighting unimportant variables. This is almost unavoidable when working with low volumes of data. The reality of regression is that you can *always* find a model that fits your training data exactly but which is typically useless for prediction. Keeping this in mind, an optimal training-test data split ratio, that is 70 training:30 test, 80 training:20 test, ..., etc., is first determined.

```{r best_p_data, echo=TRUE, warning=FALSE}
# --- test for an appropriate ratio in data partitioning
# a sequence of p's we want to test
ps <- seq(from=.30, to=.90, by=.01)
# calculate RMSEs for each p value
rmses <- sapply(ps, function(p){
  train_index <- createDataPartition(data$score, times=1, p=p, list=FALSE)
  train <- data[train_index,]
  test <- data[-train_index,]
  fit <- glm(score ~ pop2021 +
               Logged.GDP.per.capita +
               Social.support +
               Healthy.life.expectancy + 
               Freedom.to.make.life.choices + 
               Generosity + 
               Perceptions.of.corruption, 
             data = train)
  test <- test %>% mutate(pred_score = predict.glm(fit, newdata=test))
  RMSE(test$score, test$pred_score)
})
```

The tested model in the partitioning is explored after this section. Plotting `rmses` versus `p` shows a slight pattern: when you increase the training data size, our RMSE decreases. Intuitively, this makes sense. The data is quite correlated as it has already been shown and the model is being allowed to work with more training data to make better predictions in the test set. From the plot below, the lowest RMSE is `r min(rmses)` with a ratio of `r ps[which.min(rmses)]`:`r 1-ps[which.min(rmses)]`.

```{r tt, echo=FALSE}
# no real clear winner in terms of best accuracy in probabilities
plot(ps, rmses)
```

While useful in achieving a low RMSE, employing only `r 1-ps[which.min(rmses)]` percent of our data to test does not leave much in terms of prediction. Ultimately, an arbitrary value of 0.70 is chosen because RMSE seems to become more sporadic after this value. Future models in the *methods* section use 0.70 as well. This ratio is also kept when the current data is supplemented with more data.

```{r data_partitioning, echo=TRUE, warning=FALSE}
# set seed to keep partitioning consistent
set.seed(1, sample.kind = "Rounding")
# ----- Data partitioning -----
train_index <- createDataPartition(data$score, times=1, p=0.70, list=FALSE)
train <- data[train_index,]
test <- data[-train_index,]
```

With our data partitioned 0.70:0.30, a generalized linear model is fitted using the `caret` package. `score` is predicted using all seven factors. 
```{r glm_model, echo=TRUE}
# --- fit our glm model, caret::glm
fit <- glm(score ~ pop2021 + 
             Logged.GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Generosity + 
             Perceptions.of.corruption, 
           data = train)
# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))
```

```{r glm_results, echo=FALSE}
# show top five observations
results %>%
  select(happinessRank, country, score, pred_score) %>%
  head()
# show bottom five observations
results %>%
  select(happinessRank, country, score, pred_score) %>%
  tail()
```

The top five and bottom five observations are shown above compared with the actual score. The `results` data frame is plotted below with a line of best fit in blue and a reference line in red at `y = x`. If the model was to work perfectly, the line of best fit would follow the reference line because each predicted score would be equal to the score.

```{r glm_plot, echo=TRUE}
# plot predicted scores vs actual scores
# also plot y = x line
ggplot(data = results, aes(score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

Even though results have been shown, it is worth mentioning the use of RMSE instead of other success indicators. RMSE suggests how close (or far) your predicted values are from the actual data you are attempting to model. The use of a success measure for this model, and others in the methods section, is to understand the accuracy and precision of the model's predictions. For this reason, RMSE is used as a success metric over alternatives. The RMSE of this model is `r RMSE(results$Score, results$pred_score)`. The coefficients of the fitted model are shown below for completeness:

```{r mod2_coeff, echo=FALSE}
# save model 2 RMSE
mod2_rmse_gen_pop_corruption <- RMSE(results$score, results$pred_score)
# save coefficients to use in equation
c <- coefficients(fit)
c[] <- lapply(c, round, 3)
# print coefficients of fitted model
fit$coefficients
```

The following equation is the final model equation. Using these coefficients and the following notation predicted score = $\hat{y}$, GDP per capita score = $x_{GDP}$, Social Support score = $x_{SS}$, Life Expectancy score = $x_{HEA}$, Freedom score = $x_{FRE}$, Generosity score = $x_{GEN}$, Truth score = $x_{TRU}$.

\begin{equation}
  \hat{y} = `r c[1]` + `r c[2]` x_{GDP} + `r c[3]` x_{SS} + `r c[4]` x_{HEA} + `r c[5]` x_{FRE} + `r c[6]` x_{GEN} + `r c[7]` x_{TRU} 
\end{equation}



#### Removing `Generosity`

In an attempt to improve the model, we can remove the generosity component because early evaluations pointed to it being the least correlated factor. The previously partitioned data (`p = 0.70`) is used in this model as well.

```{r mod2_run_no generosity, echo=TRUE}
# fit model without generosity
fit <- glm(score ~ pop2021 + 
             Logged.GDP.per.capita +
             Social.support +
             Healthy.life.expectancy + 
             Freedom.to.make.life.choices + 
             Perceptions.of.corruption, 
           data = train)
# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit, newdata=test))
# plot predicted scores vs actual scores
ggplot(data = results, aes(score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same format as before.

```{r mod2_save_no genersity, echo=FALSE}
# save rmse and coefficients
mod2_rmse_nogen <- RMSE(results$score, results$pred_score)
c <- coefficients(fit)
c[] <- lapply(c, round, 3)
# print coefficients of fitted model
fit$coefficients
```

#### Removing Population `pop2021`

In an attempt to improve the model, we can remove the population component because early evaluations pointed to it being the least correlated factor. The previously partitioned data (`p = 0.70`) is used in this model as well.

```{r mod2_run_no population, echo=TRUE}
# fit model without population
fit1 <- glm(score ~ Logged.GDP.per.capita +
                    Social.support +
                    Healthy.life.expectancy + 
                    Freedom.to.make.life.choices + 
                    Generosity +
                    Perceptions.of.corruption, 
                    data = train)
                   
# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit1, newdata=test))
# plot predicted scores vs actual scores
ggplot(data = results, aes(score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same format as before.

```{r mod2_save_nogen, echo=FALSE}
# save rmse and coefficients
mod2_rmse_nopop <- RMSE(results$score, results$pred_score)
c <- coefficients(fit1)
c[] <- lapply(c, round, 3)
# print coefficients of fitted model
fit1$coefficients
```

#### Removing courruptiion `Perceptions.of.corruption`
In an attempt to improve the model, we can remove the Perceptions.of.corruption component because early evaluations pointed to it being the least correlated factor. The previously partitioned data (`p = 0.70`) is used in this model as well.

```{r mod2_run_nocorruption, echo=TRUE}
# fit model without corruption
fit2 <- glm(score ~ pop2021 +
                    Logged.GDP.per.capita +
                    Social.support +
                    Healthy.life.expectancy + 
                    Freedom.to.make.life.choices + 
                    Generosity, 
                    data = train)
                   
# add predicted scores to a 'results' data frame
results <- test %>% 
  mutate(pred_score = predict.glm(fit2, newdata=test))
# plot predicted scores vs actual scores
ggplot(data = results, aes(score, pred_score)) + 
  geom_point(color='black') +
  geom_smooth(method = "lm", se = TRUE) +
  geom_abline(color='red')
```

This model yields a RMSE of `r RMSE(results$Score, results$pred_score)`. The model coefficients are shown below along with the model equation following the same format as before.

```{r mod2_save_nocorruption, echo=FALSE}
# save rmse and coefficients
mod2_rmse_nocorruption <- RMSE(results$score, results$pred_score)
c <- coefficients(fit2)
c[] <- lapply(c, round, 3)
# print coefficients of fitted model
fit2$coefficients
```

## Results

The results of our five models are shown in the table below. It is clearly shown that the best model in terms of RMSE is the sum of factors model. Even though the data seemed incomplete with dystopian residuals, taking the sum of the factor and the standard dystopian scores yield the closest score predictions.

```{r compile_rmses, include=FALSE}
# compiles the list of RMSEs
rmse_list <- list(mod1_rmse,
                  mod2_rmse_gen_pop_corruption,
                  mod2_rmse_nogen,
                  mod2_rmse_nopop,
                  mod2_rmse_nocorruption)
rmse_list[] <- lapply(rmse_list, round, 3)
```

| Method                              | RMSE              |
|-------------------------------------|-------------------|
| Sum of Factors Model                | `r rmse_list[1]`  |
| GLM Model 2019                      | `r rmse_list[2]`  |
| GLM Model 2018/2019                 | `r rmse_list[4]`  |
| GLM Model - No Generosity 2019      | `r rmse_list[3]`  |
| GLM Model - No Generosity 2018/2019 | `r rmse_list[5]`  |

For completeness, it may be useful to check the summation model with the full 2018-2019 data set. This is shown below and reported in the final table.


```{r sum_factors_model_full, echo=TRUE, warning=FALSE}
# save RMSE for the first model
mod1_rmse_full <- RMSE(sum_model$Score, sum_model$pred_score)
```
```{r mod1_full_rmse_save, include=FALSE}
# recompiles the list of RMSEs
rmse_list <- list(mod1_rmse,
                  mod2_rmse_gen_pop_corruption,
                  mod2_rmse_nogen,
                  mod2_rmse_nopop,
                  mod2_rmse_nocorruption,
                  mod1_rmse_full)

rmse_list[] <- lapply(rmse_list, round, 3)
```

It is clearly seen that the best model is the summation model. Interestingly the GLM models do not improve when the data set size doubles. This may be due to overfitting in smaller data size that does not occur in the larger data.

| Method                              | RMSE              |
|-------------------------------------|-------------------|
| Sum of Factors Model                | `r rmse_list[1]`  |
| Sum of Factors Model 2018/2019      | `r rmse_list[6]`  |
| GLM Model 2019                      | `r rmse_list[2]`  |
| GLM Model 2018/2019                 | `r rmse_list[4]`  |
| GLM Model - No Generosity 2019      | `r rmse_list[3]`  |
| GLM Model - No Generosity 2018/2019 | `r rmse_list[5]`  |



## Conclusion

Machine learning algorithms have become a paramount tool in computer science and analysis. This report explores this use with regression analysis on a dataset of national happiness. Results are found that are rather unexpected: a glaring error in the data did not impede the simplest model, increasing training data set size did not decrease the error, and reducing the effectiveness of uncorrelated variables did not decrease the error.

This report felt limited in the amount of data being trained and tested. If more consistent data was available, it is my belief that the summation model would out perform the GLM models. The omitted residuals left a hole in the data that may have provided more insight into the validity of the summation model. The discussion boards were helpful in this respect by providing insight on the purpose of the data. Further work on a project such as this would include calculating and using dystopian residuals for data sets where they were lost, combining previous years reports, and comparing more adaptive models on the large data sets.

